# -*- coding: utf-8 -*-
"""phonedatamining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T-Zxncb1geppiGldkSI_cPVIahy3KJaU

# Identifying the Problem

Clearly define the specific question or problem related to the impact of mobile phones on students' health.

# Data Understanding

# Data Collection
First, load the data from your CSV file.
"""

# Step : Load the dataset
# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import silhouette_score, r2_score

# Load the dataset from your Google Colab environment
file_path = '/content/Impact_of_Mobile_Phone_on_Students_Health.csv'
df = pd.read_csv(file_path)

# Display the first few rows of the dataset
print("Initial Dataset Overview:")
print(df.info())  # Shows data types and missing values
print(df.describe())  # Summary statistics
print(df.head())  # Display the first few rows\

"""# Data Exploration
Explore the structure and types of data in the dataset.
"""

# Check the shape and types of data
print("Dataset Shape:", df.shape)
print("Data Types:\n", df.dtypes)

# Get basic summarization statistics
print("\nBasic Statistics Summary:")
print(df.describe(include='all'))

# Check for missing values
print("Missing Values:\n", df.isnull().sum())

# Visualize missing values
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

"""Data Preprocessing

Cleaning
Remove inconsistencies or errors, handle missing values, and remove duplicates.
"""

#  Data Cleaning
# Impute missing values (using the most frequent strategy)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Check for duplicate records
duplicates = df_imputed.duplicated().sum()
print(f"\nNumber of duplicate records: {duplicates}")

# Remove duplicates
df_imputed = df_imputed.drop_duplicates()
print(f"Number of duplicates removed: {df.duplicated().sum()}")

"""Transforming Categorical Variables
Convert categorical variables to numerical representations.
"""

# Convert categorical variables to numerical
from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for column in df_imputed.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df_imputed[column] = le.fit_transform(df_imputed[column])
    label_encoders[column] = le
print("Categorical columns converted to numerical.")

"""Reduction
Dimensionality reduction or attribute subset selection can be implemented using techniques like PCA.
"""

#  Dimensionality Reduction with PCA
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Use df_imputed instead of df to ensure 'Health rating' is numerical
health_rating_data = df_imputed[['Health rating']]

# Scale the data before applying PCA
scaler = StandardScaler()
scaled_data = scaler.fit_transform(health_rating_data)

pca = PCA(n_components=1)
reduced_df = pca.fit_transform(scaled_data)
print("PCA Reduction Completed on 'Health rating'") # Use scaled data here

# Convert string columns to numerical representations
# First you need to identify columns with string values
for col in df_imputed.columns:
  if df_imputed[col].dtype == 'object':
    print(f"Column {col} contains string values.")

"""Normalizing Numerical Data
Normalize numerical columns to ensure uniformity.
"""

# Normalize numerical columns
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_columns = df_imputed.select_dtypes(include=[np.number]).columns
df_imputed[numerical_columns] = scaler.fit_transform(df_imputed[numerical_columns])

# Display the normalized dataset
print("\nNormalized Data (first few rows):")
print(df_imputed.head())

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
encoder = LabelEncoder()

# Fit the encoder to the 'Health rating' column and transform it
df['Health rating'] = encoder.fit_transform(df['Health rating'])

# Now you can apply MinMaxScaler
scaler = MinMaxScaler()
df[['Health rating']] = scaler.fit_transform(df[['Health rating']])

"""Handle missing values (median, mean, and mode)."""

# Handle missing values (e.g., filling with median, mean, or mode)
df = df.fillna(df.median(numeric_only=True))

# Drop duplicate records
df = df.drop_duplicates()

# Assuming that 'df' is the DataFrame you intended to copy
Original_data = df.copy()

# Check the column names of the DataFrame
print(df.columns)
df.info()

"""Data Mining

Clustering
Implement clustering techniques to group similar data points.
"""

#  Apply Clustering
from sklearn.cluster import KMeans

# Clustering with KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
df_imputed['Cluster'] = kmeans.fit_predict(df_imputed[numerical_columns])
print("\nClustering completed. Cluster labels added to the DataFrame.")

"""Evaluate Clustering Performance
Evaluate the performance of the clustering method.
"""

#  Evaluate Clustering Performance
from sklearn.metrics import silhouette_score

print("\nCluster Centers:\n", kmeans.cluster_centers_)
score = silhouette_score(df_imputed[numerical_columns], df_imputed['Cluster'])
print(f"\nSilhouette Score: {score}")

"""Model Training and Evaluation

Splitting Data
Split the dataset into training and testing sets.
"""

df_imputed.columns

# Data Splitting for model training
from sklearn.model_selection import train_test_split

y_col = 'Daily usages'
X = df_imputed.drop(columns=[y_col])
y = df_imputed[y_col]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Data split into training and testing sets.")

"""Evaluate resulting model/patterns."""

# Model Training and Cross-validation
# Model evaluation
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression

model = LinearRegression()
cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')
print(f"Cross-validation scores (negative MSE): {cv_scores}")
print(f"Mean CV score (negative MSE): {cv_scores.mean()}")

# Hyperparameter tuning (GridSearchCV example)
from sklearn.model_selection import GridSearchCV

param_grid = {'fit_intercept': [True, False], 'positive': [True, False]}
grid_search = GridSearchCV(estimator=LinearRegression(), param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score (negative MSE): {grid_search.best_score_}")

"""Clustering"""

# Visualize Clustering Results
sns.pairplot(df_imputed, hue='Cluster')
plt.show()

"""Linear Regression"""

# Train a Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

regressor = RandomForestRegressor(random_state=42)
regressor.fit(X_train, y_train)
r2 = r2_score(y_test, regressor.predict(X_test))
print(f'Random Forest Regressor R-squared: {r2}')

"""Visualization of Results

Feature Importance
Visualize the importance of each feature in the model.
"""

# Feature Importance Visualization
importances = regressor.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importance')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

print(df.columns)
column_name = 'Daily usages'

sns.histplot(df['Daily usages'], bins=30, kde=True)
plt.title('Distribution of Mobile Phone Usage Time')
plt.xlabel('Daily Usage (Hours)')
plt.ylabel('Frequency')
plt.show()

# Visualize the distribution of key variables
sns.countplot(x='Age', data=df)
plt.title('Gender Distribution')
plt.show()

sns.countplot(x='Daily usages', data=df)
plt.title('Mobile Operating System Distribution')
plt.show()

sns.countplot(x='Mobile phone use for education', data=df)
plt.title('Mobile Operating System Distribution')
plt.show()

imputedData = df  # Rename df to imputedData
sns.displot(imputedData["Helpful for studying"])

import matplotlib.pyplot as plt
import pandas as pd

# Load your data into the imputedData variable
imputedData = pd.read_csv('/content/Impact_of_Mobile_Phone_on_Students_Health.csv')

# Print the actual column names to inspect them
print(imputedData.columns)

# Access columns using their actual names (replace with actual names from the output above)
plt.figure(figsize=(6, 6))
plt.pie(imputedData['Age'].value_counts(),  # Replace 'Age' with the actual column name
        labels=imputedData['Age'].value_counts().index,  # Replace 'Age' with the actual column name
        autopct='%1.1f%%',
        startangle=90)
plt.title('Distribution of Age')  # Update the title accordingly
plt.show()


import matplotlib.pyplot as plt
import pandas as pd

# Repeat for other categorical columns as needed
# For example:
plt.figure(figsize=(6, 6))

# Access the column using its actual name from the DataFrame
plt.pie(imputedData['Mobile phone use for education'].value_counts(),
        labels=imputedData['Mobile phone use for education'].value_counts().index,
        autopct='%1.1f%%',
        startangle=90)

# Update the title to reflect the actual column name
plt.title('Distribution of Mobile phone use for education')
plt.show()

# Additional Visualizations
plt.figure(figsize=(10, 8))
sns.boxplot(data=df_imputed)
plt.xticks(rotation=90)
plt.title("Outliers in the Dataset")
plt.show()

"""Save the Cleaned Data Save the cleaned dataset for future reference."""

# Save Cleaned Dataset
df_imputed.to_csv('Cleaned_Impact_of_Mobile_Phone_on_Students_Health.csv', index=False)
print("Cleaned data saved to 'Cleaned_Impact_of_Mobile_Phone_on_Students_Health.csv'.")